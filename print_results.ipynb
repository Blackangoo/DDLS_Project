{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximal accuracy\n",
    "\n",
    "### each algorithm is run at least 10 times and then the results are averaged and stored in the XXX_avg.h5 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_read_data(alg, folder=\"\"):\n",
    "    \"\"\"\n",
    "    Read training accuracy, training loss, and global accuracy from an HDF5 file.\n",
    "\n",
    "    Parameters:\n",
    "    alg (str): The name of the algorithm.\n",
    "    folder (str): The folder name where the HDF5 file is located.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing training accuracy, training loss, and global accuracy.\n",
    "    \"\"\"\n",
    "    \n",
    "    path = os.path.join(\"./results_2\", folder, '{}.h5'.format(alg))\n",
    "    hf = h5py.File(path, 'r')\n",
    "    rs_glob_acc = np.array(hf.get('rs_glob_acc')[:])\n",
    "    rs_train_acc = np.array(hf.get('rs_train_acc')[:])\n",
    "    rs_train_loss = np.array(hf.get('rs_train_loss')[:])\n",
    "    return rs_train_acc, rs_train_loss, rs_glob_acc\n",
    "\n",
    "def get_training_data_value(num_users=100, loc_ep1=5, Numb_Glob_Iters=10, lamb=[], learning_rate=[],beta=[],algorithms_list=[], batch_size=[], dataset=\"\", k= [] , personal_learning_rate = [], folder=\"\"):\n",
    "    \"\"\"\n",
    "    Get training accuracy, training loss, and global accuracy data from HDF5 files.\n",
    "\n",
    "    Parameters:\n",
    "    num_users (int): Number of users.\n",
    "    loc_ep1 (int): Number of local epochs.\n",
    "    Numb_Glob_Iters (int): Number of global iterations.\n",
    "    lamb (list): List of lambda values.\n",
    "    learning_rate (list): List of learning rates.\n",
    "    beta (list): List of beta values.\n",
    "    algorithms_list (list): List of algorithm names.\n",
    "    batch_size (list): List of batch sizes.\n",
    "    dataset (str): Name of the dataset.\n",
    "    k (list): List of k values.\n",
    "    personal_learning_rate (list): List of personal learning rates.\n",
    "    folder (str): The folder name where the HDF5 files are located.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing global accuracy, training accuracy, and training loss data.\n",
    "    \"\"\"\n",
    "\n",
    "    Numb_Algs = len(algorithms_list)\n",
    "    train_acc = np.zeros((Numb_Algs, Numb_Glob_Iters))\n",
    "    train_loss = np.zeros((Numb_Algs, Numb_Glob_Iters))\n",
    "    glob_acc = np.zeros((Numb_Algs, Numb_Glob_Iters))\n",
    "    algs_lbl = algorithms_list.copy()\n",
    "\n",
    "    for i in range(Numb_Algs):\n",
    "        string_learning_rate = str(learning_rate[i])  \n",
    "        string_learning_rate = string_learning_rate + \"_\" +str(beta[i]) + \"_\" +str(lamb[i])\n",
    "        if(algorithms_list[i] == \"pFedMe\" or algorithms_list[i] == \"pFedMe_p\"):\n",
    "            algorithms_list[i] = algorithms_list[i] + \"_\" + string_learning_rate + \"_\" + str(num_users) + \"u\" + \"_\" + str(batch_size[i]) + \"b\" + \"_\" +str(loc_ep1[i]) + \"_\"+ str(k[i])  + \"_\"+ str(personal_learning_rate[i])\n",
    "        else:\n",
    "            algorithms_list[i] = algorithms_list[i] + \"_\" + string_learning_rate + \"_\" + str(num_users) + \"u\" + \"_\" + str(batch_size[i]) + \"b\"  \"_\" +str(loc_ep1[i])\n",
    "        train_acc[i, :], train_loss[i, :], glob_acc[i, :] = np.array(\n",
    "            simple_read_data(dataset +\"_\"+ algorithms_list[i] + \"_avg\", folder))[:, :Numb_Glob_Iters]\n",
    "        algorithms_list[i] = algs_lbl[i]\n",
    "    return glob_acc, train_acc, train_loss\n",
    "\n",
    "def get_max_value_index(num_users=100, loc_ep1=5, Numb_Glob_Iters=10, lamb=[], learning_rate=[],beta=[],algorithms_list=[], batch_size=[], dataset=\"\", k= [] , personal_learning_rate = [], folder=\"\"):\n",
    "    \"\"\"\n",
    "    Print the maximum testing accuracy and its index for each algorithm.\n",
    "\n",
    "    Parameters:\n",
    "    num_users (int): Number of users.\n",
    "    loc_ep1 (int): Number of local epochs.\n",
    "    Numb_Glob_Iters (int): Number of global iterations.\n",
    "    lamb (list): List of lambda values.\n",
    "    learning_rate (list): List of learning rates.\n",
    "    beta (list): List of beta values.\n",
    "    algorithms_list (list): List of algorithm names.\n",
    "    batch_size (list): List of batch sizes.\n",
    "    dataset (str): Name of the dataset.\n",
    "    k (list): List of k values.\n",
    "    personal_learning_rate (list): List of personal learning rates.\n",
    "    folder (str): The folder name where the HDF5 files are located.\n",
    "    \"\"\"\n",
    "\n",
    "    results = []\n",
    "\n",
    "    Numb_Algs = len(algorithms_list)\n",
    "    glob_acc, train_acc, train_loss = get_training_data_value(num_users, loc_ep1, Numb_Glob_Iters, lamb, learning_rate, beta, algorithms_list, batch_size, dataset, k, personal_learning_rate, folder)\n",
    "    \n",
    "\n",
    "\n",
    "    for i in range(Numb_Algs):\n",
    "        '''print(\"Algorithm: \", algorithms_list[i],\n",
    "            \"Max testing Accurancy: \", glob_acc[i].max(), \n",
    "            \"Index: \", np.argmax(glob_acc[i]),\n",
    "            \"local update:\", loc_ep1[i])'''\n",
    "        results.append({\n",
    "            \"Algorithm\": algorithms_list[i],\n",
    "            \"Folder\": folder,\n",
    "            \"Max testing Accuracy\": glob_acc[i].max(),\n",
    "            \"Index\": np.argmax(glob_acc[i]),\n",
    "            #\"local update\": loc_ep1\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Algorithm</th>\n",
       "      <th>Folder</th>\n",
       "      <th>Max testing Accuracy</th>\n",
       "      <th>Index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pFedMe_p</td>\n",
       "      <td>results_DNN</td>\n",
       "      <td>0.967495</td>\n",
       "      <td>255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pFedMe</td>\n",
       "      <td>results_DNN</td>\n",
       "      <td>0.962392</td>\n",
       "      <td>768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PerAvg_p</td>\n",
       "      <td>results_DNN</td>\n",
       "      <td>0.941550</td>\n",
       "      <td>794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FedAvg</td>\n",
       "      <td>results_DNN</td>\n",
       "      <td>0.960448</td>\n",
       "      <td>570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pFedMe_p</td>\n",
       "      <td>results_MLR</td>\n",
       "      <td>0.939390</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>pFedMe</td>\n",
       "      <td>results_MLR</td>\n",
       "      <td>0.919438</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>PerAvg_p</td>\n",
       "      <td>results_MLR</td>\n",
       "      <td>0.933477</td>\n",
       "      <td>470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>FedAvg</td>\n",
       "      <td>results_MLR</td>\n",
       "      <td>0.925000</td>\n",
       "      <td>743</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Algorithm       Folder  Max testing Accuracy  Index\n",
       "0  pFedMe_p  results_DNN              0.967495    255\n",
       "1    pFedMe  results_DNN              0.962392    768\n",
       "2  PerAvg_p  results_DNN              0.941550    794\n",
       "3    FedAvg  results_DNN              0.960448    570\n",
       "4  pFedMe_p  results_MLR              0.939390     76\n",
       "5    pFedMe  results_MLR              0.919438    250\n",
       "6  PerAvg_p  results_MLR              0.933477    470\n",
       "7    FedAvg  results_MLR              0.925000    743"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algorithms = [\"pFedMe_p\", \"pFedMe\", \"PerAvg_p\", \"FedAvg\"]\n",
    "dataset = \"Mnist\"\n",
    "Numb_Glob_Iters = 800\n",
    "folders = [\"results_DNN\", \"results_MLR\"]\n",
    "num_users = 5\n",
    "\n",
    "params = {\n",
    "    \"results_DNN\": {\n",
    "        \"learning_rates\": [0.01, 0.01, 0.02, 0.02],\n",
    "        \"betas\": [2.0, 2.0, 0.001, 1.0],\n",
    "        \"lambdas\": [30, 30, 15, 15],\n",
    "        \"personal_learning_rate\": [0.05, 0.05, 0.05, 0.05],\n",
    "        \"local_epochs\": [20, 20, 20, 20],\n",
    "        \"K\": [5, 5, 5, 5],\n",
    "        \"batch_sizes\": [20, 20, 20, 20]\n",
    "    },\n",
    "    \"results_MLR\": {\n",
    "        \"learning_rates\": [0.01, 0.01, 0.03, 0.02],\n",
    "        \"betas\": [2.0, 2.0, 0.003, 1.0],\n",
    "        \"lambdas\": [15, 15, 15, 15],\n",
    "        \"personal_learning_rate\": [0.1, 0.1, 0.1, 0.1],\n",
    "        \"local_epochs\": [20, 20, 20, 20],\n",
    "        \"K\": [5, 5, 5, 5],\n",
    "        \"batch_sizes\": [20, 20, 20, 20]\n",
    "    }\n",
    "}\n",
    "\n",
    "results_all = []\n",
    "\n",
    "for folder in folders:\n",
    "        results_df = get_max_value_index(\n",
    "            num_users=num_users,\n",
    "            loc_ep1=params[folder][\"local_epochs\"],\n",
    "            Numb_Glob_Iters=Numb_Glob_Iters,\n",
    "            lamb=params[folder][\"lambdas\"],\n",
    "            learning_rate=params[folder][\"learning_rates\"],\n",
    "            beta=params[folder][\"betas\"],\n",
    "            algorithms_list=algorithms,\n",
    "            batch_size=params[folder][\"batch_sizes\"],\n",
    "            dataset=dataset,\n",
    "            k=params[folder][\"K\"],\n",
    "            personal_learning_rate=params[folder][\"personal_learning_rate\"],\n",
    "            folder=folder\n",
    "        )\n",
    "        results_all.append(results_df)\n",
    "\n",
    "# Concatenate all DataFrames\n",
    "all_results_df = pd.concat(results_all, ignore_index=True)\n",
    "\n",
    "all_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outputs the maximal accuracy for each file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_read_data(alg, folder=\"\"):\n",
    "    \"\"\"\n",
    "    Read training accuracy, training loss, and global accuracy from an HDF5 file.\n",
    "\n",
    "    Parameters:\n",
    "    alg (str): The name of the algorithm.\n",
    "    folder (str): The folder name where the HDF5 file is located.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing training accuracy, training loss, and global accuracy.\n",
    "    \"\"\"\n",
    "    \n",
    "    path = os.path.join(\"./results_2\", folder, '{}.h5'.format(alg))\n",
    "    hf = h5py.File(path, 'r')\n",
    "    rs_glob_acc = np.array(hf.get('rs_glob_acc')[:])\n",
    "    rs_train_acc = np.array(hf.get('rs_train_acc')[:])\n",
    "    rs_train_loss = np.array(hf.get('rs_train_loss')[:])\n",
    "    return rs_train_acc, rs_train_loss, rs_glob_acc\n",
    "\n",
    "def get_all_training_data_value(num_users=100, loc_ep1=5, Numb_Glob_Iters=10, lamb=0, learning_rate=0, beta=0, algorithms=\"\", batch_size=0, dataset=\"\", k=0, personal_learning_rate=0, times=10, folder=\"\"):\n",
    "    \"\"\"\n",
    "    Get training accuracy, training loss, and global accuracy data from HDF5 files.\n",
    "\n",
    "    Parameters:\n",
    "    num_users (int): Number of users.\n",
    "    loc_ep1 (int): Number of local epochs.\n",
    "    Numb_Glob_Iters (int): Number of global iterations.\n",
    "    lamb (float): Lambda value.\n",
    "    learning_rate (float): Learning rate.\n",
    "    beta (float): Beta value.\n",
    "    algorithms (str): Name of the algorithm.\n",
    "    batch_size (int): Batch size.\n",
    "    dataset (str): Name of the dataset.\n",
    "    k (int): k value.\n",
    "    personal_learning_rate (float): Personal learning rate.\n",
    "    times (int): Number of times to repeat the experiment.\n",
    "    folder (str): The folder name where the HDF5 files are located.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing global accuracy, training accuracy, and training loss data.\n",
    "    \"\"\"\n",
    "\n",
    "    train_acc = np.zeros((times, Numb_Glob_Iters))\n",
    "    train_loss = np.zeros((times, Numb_Glob_Iters))\n",
    "    glob_acc = np.zeros((times, Numb_Glob_Iters))\n",
    "    algorithms_list = [algorithms] * times\n",
    "\n",
    "    for i in range(times):\n",
    "        string_learning_rate = str(learning_rate)\n",
    "        string_learning_rate = string_learning_rate + \\\n",
    "            \"_\" + str(beta) + \"_\" + str(lamb)\n",
    "        if(algorithms == \"pFedMe\" or algorithms == \"pFedMe_p\"):\n",
    "            algorithms_list[i] = algorithms_list[i] + \"_\" + string_learning_rate + \"_\" + \\\n",
    "                str(num_users) + \"u\" + \"_\" + str(batch_size) + \"b\" + \"_\" + \\\n",
    "                str(loc_ep1) + \"_\" + str(k) + \"_\" + \\\n",
    "                str(personal_learning_rate) + \"_\" + str(i)\n",
    "        else:\n",
    "            algorithms_list[i] = algorithms_list[i] + \"_\" + string_learning_rate + \"_\" + \\\n",
    "                str(num_users) + \"u\" + \"_\" + str(batch_size) + \"b\" + \\\n",
    "                \"_\" + str(loc_ep1) + \"_\" + str(i)\n",
    "\n",
    "        train_acc[i, :], train_loss[i, :], glob_acc[i, :] = np.array(\n",
    "            simple_read_data(dataset + \"_\" + algorithms_list[i], folder))[:, :Numb_Glob_Iters]\n",
    "    return glob_acc, train_acc, train_loss\n",
    "\n",
    "\n",
    "def get_max_value_index_all(num_users=100, loc_ep1=5, Numb_Glob_Iters=10, lamb=0, learning_rate=0, beta=0, algorithms=\"\", batch_size=0, dataset=\"\", k=0, personal_learning_rate=0, times=10, folder=\"\"):\n",
    "    \"\"\"\n",
    "    Get maximum testing accuracy, its index, mean accuracy, and variance for each experiment.\n",
    "\n",
    "    Parameters:\n",
    "    num_users (int): Number of users.\n",
    "    loc_ep1 (int): Number of local epochs.\n",
    "    Numb_Glob_Iters (int): Number of global iterations.\n",
    "    lamb (float): Lambda value.\n",
    "    learning_rate (float): Learning rate.\n",
    "    beta (float): Beta value.\n",
    "    algorithms (str): Name of the algorithm.\n",
    "    batch_size (int): Batch size.\n",
    "    dataset (str): Name of the dataset.\n",
    "    k (int): k value.\n",
    "    personal_learning_rate (float): Personal learning rate.\n",
    "    times (int): Number of times to repeat the experiment.\n",
    "    folder (str): The folder name where the HDF5 files are located.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: A DataFrame containing the results.\n",
    "    \"\"\"\n",
    "\n",
    "    results = []\n",
    "\n",
    "    glob_acc, _, _ = get_all_training_data_value(\n",
    "        num_users, loc_ep1, Numb_Glob_Iters, lamb, learning_rate, beta, algorithms, batch_size, dataset, k, personal_learning_rate, times, folder)\n",
    "\n",
    "    for i in range(times):\n",
    "        results.append({\n",
    "            \"Algorithm\": algorithms,\n",
    "            \"Folder\": folder,\n",
    "            \"Max testing Accuracy\": glob_acc[i].max(),\n",
    "            \"Index\": np.argmax(glob_acc[i]),\n",
    "            #\"local update\": loc_ep1\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Algorithm</th>\n",
       "      <th>Folder</th>\n",
       "      <th>Max testing Accuracy</th>\n",
       "      <th>Index</th>\n",
       "      <th>Mean Accuracy</th>\n",
       "      <th>Variance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FedAvg</td>\n",
       "      <td>results_DNN</td>\n",
       "      <td>0.962743</td>\n",
       "      <td>556</td>\n",
       "      <td>0.961744</td>\n",
       "      <td>2.437710e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FedAvg</td>\n",
       "      <td>results_MLR</td>\n",
       "      <td>0.929266</td>\n",
       "      <td>780</td>\n",
       "      <td>0.928483</td>\n",
       "      <td>4.284214e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PerAvg_p</td>\n",
       "      <td>results_DNN</td>\n",
       "      <td>0.942765</td>\n",
       "      <td>795</td>\n",
       "      <td>0.941712</td>\n",
       "      <td>6.551852e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PerAvg_p</td>\n",
       "      <td>results_MLR</td>\n",
       "      <td>0.935745</td>\n",
       "      <td>470</td>\n",
       "      <td>0.934746</td>\n",
       "      <td>5.353243e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pFedMe</td>\n",
       "      <td>results_DNN</td>\n",
       "      <td>0.965173</td>\n",
       "      <td>782</td>\n",
       "      <td>0.963688</td>\n",
       "      <td>4.413794e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>pFedMe</td>\n",
       "      <td>results_MLR</td>\n",
       "      <td>0.924406</td>\n",
       "      <td>433</td>\n",
       "      <td>0.923785</td>\n",
       "      <td>3.733503e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>pFedMe_p</td>\n",
       "      <td>results_DNN</td>\n",
       "      <td>0.970572</td>\n",
       "      <td>184</td>\n",
       "      <td>0.969546</td>\n",
       "      <td>5.636698e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>pFedMe_p</td>\n",
       "      <td>results_MLR</td>\n",
       "      <td>0.944114</td>\n",
       "      <td>39</td>\n",
       "      <td>0.942414</td>\n",
       "      <td>1.620551e-06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Algorithm       Folder  Max testing Accuracy  Index  Mean Accuracy  \\\n",
       "0    FedAvg  results_DNN              0.962743    556       0.961744   \n",
       "1    FedAvg  results_MLR              0.929266    780       0.928483   \n",
       "2  PerAvg_p  results_DNN              0.942765    795       0.941712   \n",
       "3  PerAvg_p  results_MLR              0.935745    470       0.934746   \n",
       "4    pFedMe  results_DNN              0.965173    782       0.963688   \n",
       "5    pFedMe  results_MLR              0.924406    433       0.923785   \n",
       "6  pFedMe_p  results_DNN              0.970572    184       0.969546   \n",
       "7  pFedMe_p  results_MLR              0.944114     39       0.942414   \n",
       "\n",
       "       Variance  \n",
       "0  2.437710e-07  \n",
       "1  4.284214e-07  \n",
       "2  6.551852e-07  \n",
       "3  5.353243e-07  \n",
       "4  4.413794e-07  \n",
       "5  3.733503e-07  \n",
       "6  5.636698e-07  \n",
       "7  1.620551e-06  "
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algorithms = [\"pFedMe_p\", \"pFedMe\", \"PerAvg_p\", \"FedAvg\"]\n",
    "dataset = \"Mnist\"\n",
    "Numb_Glob_Iters = 800\n",
    "folders = [\"results_DNN\", \"results_MLR\"]\n",
    "\n",
    "params = {\n",
    "    \"results_DNN\": {\n",
    "        \"learning_rates\": [0.01, 0.01, 0.02, 0.02],\n",
    "        \"betas\": [2.0, 2.0, 0.001, 1.0],\n",
    "        \"lambdas\": [30, 30, 15, 15],\n",
    "        \"personal_learning_rate\": [0.05, 0.05, 0.05, 0.05],\n",
    "        \"local_epochs\": [20, 20, 20, 20],\n",
    "        \"K\": [5, 5, 5, 5],\n",
    "        \"batch_sizes\": [20, 20, 20, 20]\n",
    "    },\n",
    "    \"results_MLR\": {\n",
    "        \"learning_rates\": [0.01, 0.01, 0.03, 0.02],\n",
    "        \"betas\": [2.0, 2.0, 0.003, 1.0],\n",
    "        \"lambdas\": [15, 15, 15, 15],\n",
    "        \"personal_learning_rate\": [0.1, 0.1, 0.1, 0.1],\n",
    "        \"local_epochs\": [20, 20, 20, 20],\n",
    "        \"K\": [5, 5, 5, 5],\n",
    "        \"batch_sizes\": [20, 20, 20, 20]\n",
    "    }\n",
    "}\n",
    "\n",
    "results_all = []\n",
    "\n",
    "for folder in folders:\n",
    "    for i in range(len(algorithms)):\n",
    "        results_df = get_max_value_index_all(\n",
    "            num_users=num_users,\n",
    "            loc_ep1=params[folder][\"local_epochs\"][i],\n",
    "            Numb_Glob_Iters=Numb_Glob_Iters,\n",
    "            lamb=params[folder][\"lambdas\"][i],\n",
    "            learning_rate=params[folder][\"learning_rates\"][i],\n",
    "            beta=params[folder][\"betas\"][i],\n",
    "            algorithms=algorithms[i],\n",
    "            batch_size=params[folder][\"batch_sizes\"][i],\n",
    "            dataset=dataset,\n",
    "            folder=folder,\n",
    "            k=params[folder][\"K\"][i],\n",
    "            personal_learning_rate=params[folder][\"personal_learning_rate\"][i]\n",
    "        )\n",
    "        results_all.append(results_df)\n",
    "\n",
    "# Concatenate all DataFrames\n",
    "all_results_df = pd.concat(results_all, ignore_index=True)\n",
    "\n",
    "# Get only the rows with maximum accuracy for each algorithm and each folder\n",
    "max_results_df = all_results_df.loc[all_results_df.groupby([\"Algorithm\", \"Folder\"])[\"Max testing Accuracy\"].idxmax()]\n",
    "\n",
    "# Get the mean and variance for each algorithm and each folder\n",
    "mean_var_results_df = all_results_df.groupby([\"Algorithm\", \"Folder\"]).agg(\n",
    "    {\"Max testing Accuracy\": [\"mean\", \"var\"]}\n",
    ")\n",
    "\n",
    "# Rename columns\n",
    "mean_var_results_df.columns = [\"Mean Accuracy\", \"Variance\"]\n",
    "\n",
    "# Reset index\n",
    "mean_var_results_df = mean_var_results_df.reset_index()\n",
    "\n",
    "# Merge with max_results_df\n",
    "max_results_df = max_results_df.merge(mean_var_results_df, on=[\"Algorithm\", \"Folder\"])\n",
    "\n",
    "max_results_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
