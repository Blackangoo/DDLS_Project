{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_read_data(alg):\n",
    "    print(alg)\n",
    "    hf = h5py.File(\"./results/FedAvg_MLR_MNIST/\"+'{}.h5'.format(alg), 'r')\n",
    "    rs_glob_acc = np.array(hf.get('rs_glob_acc')[:])\n",
    "    rs_train_acc = np.array(hf.get('rs_train_acc')[:])\n",
    "    rs_train_loss = np.array(hf.get('rs_train_loss')[:])\n",
    "    return rs_train_acc, rs_train_loss, rs_glob_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_data_value(num_users=100, loc_ep1=5, Numb_Glob_Iters=10, lamb=[], learning_rate=[],beta=[],algorithms_list=[], batch_size=[], dataset=\"\", k= [] , personal_learning_rate = []):\n",
    "    Numb_Algs = len(algorithms_list)\n",
    "    train_acc = np.zeros((Numb_Algs, Numb_Glob_Iters))\n",
    "    train_loss = np.zeros((Numb_Algs, Numb_Glob_Iters))\n",
    "    glob_acc = np.zeros((Numb_Algs, Numb_Glob_Iters))\n",
    "    algs_lbl = algorithms_list.copy()\n",
    "    for i in range(Numb_Algs):\n",
    "        string_learning_rate = str(learning_rate[i])  \n",
    "        string_learning_rate = string_learning_rate + \"_\" +str(beta[i]) + \"_\" +str(lamb[i])\n",
    "        if(algorithms_list[i] == \"pFedMe\" or algorithms_list[i] == \"pFedMe_p\"):\n",
    "            algorithms_list[i] = algorithms_list[i] + \"_\" + string_learning_rate + \"_\" + str(num_users) + \"u\" + \"_\" + str(batch_size[i]) + \"b\" + \"_\" +str(loc_ep1[i]) + \"_\"+ str(k[i])  + \"_\"+ str(personal_learning_rate[i])\n",
    "        else:\n",
    "            algorithms_list[i] = algorithms_list[i] + \"_\" + string_learning_rate + \"_\" + str(num_users) + \"u\" + \"_\" + str(batch_size[i]) + \"b\"  \"_\" +str(loc_ep1[i])\n",
    "\n",
    "        train_acc[i, :], train_loss[i, :], glob_acc[i, :] = np.array(\n",
    "            simple_read_data(dataset +\"_\"+ algorithms_list[i] + \"_avg\"))[:, :Numb_Glob_Iters]\n",
    "        algs_lbl[i] = algs_lbl[i]\n",
    "    return glob_acc, train_acc, train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithms = [\"FedAvg\"]\n",
    "dataset = \"Mnist\"\n",
    "learning_rates = [0.02]\n",
    "betas = [1.0]\n",
    "lambdas = [15]\n",
    "batch_sizes = [20]\n",
    "num_users = 5\n",
    "local_epochs = [20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mnist_FedAvg_0.02_1.0_15_5u_20b_20_avg\n"
     ]
    }
   ],
   "source": [
    "glob_acc, train_acc, train_loss = get_training_data_value(num_users=num_users,\n",
    "                                                          loc_ep1=local_epochs,\n",
    "                                                          Numb_Glob_Iters=800,\n",
    "                                                          lamb=lambdas,\n",
    "                                                          learning_rate=learning_rates,\n",
    "                                                          beta=betas,\n",
    "                                                          algorithms_list=algorithms,\n",
    "                                                          batch_size=batch_sizes,\n",
    "                                                          dataset=dataset,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.07991361, 0.36687365, 0.49190065, 0.60915227, 0.68544816,\n",
       "        0.72788877, 0.75915227, 0.7887149 , 0.81727862, 0.83339633,\n",
       "        0.83566415, 0.84732721, 0.84697624, 0.85210583, 0.85737041,\n",
       "        0.85890929, 0.85853132, 0.86762959, 0.86943844, 0.87448704,\n",
       "        0.87424406, 0.8812635 , 0.88633909, 0.8875    , 0.88906587,\n",
       "        0.88965983, 0.88847192, 0.89114471, 0.89222462, 0.89055076,\n",
       "        0.89257559, 0.89597732, 0.89659827, 0.89173866, 0.89041577,\n",
       "        0.8949784 , 0.89230562, 0.89476242, 0.89595032, 0.89943305,\n",
       "        0.89959503, 0.89954104, 0.89897408, 0.89927106, 0.90299676,\n",
       "        0.90153888, 0.90321274, 0.90159287, 0.90315875, 0.9012689 ,\n",
       "        0.90140389, 0.90367171, 0.90356371, 0.90448164, 0.90269978,\n",
       "        0.90361771, 0.90456263, 0.90672246, 0.90639849, 0.90720842,\n",
       "        0.90712743, 0.90550756, 0.90521058, 0.90847732, 0.90988121,\n",
       "        0.91169006, 0.91198704, 0.91309395, 0.90977322, 0.90820734,\n",
       "        0.91012419, 0.91106911, 0.9112041 , 0.91217603, 0.91169006,\n",
       "        0.91295896, 0.9125    , 0.91028618, 0.90896328, 0.91031317,\n",
       "        0.91279698, 0.91517279, 0.91457883, 0.91355292, 0.91298596,\n",
       "        0.9136879 , 0.91484881, 0.9149568 , 0.91441685, 0.91401188,\n",
       "        0.91241901, 0.91425486, 0.9125    , 0.91409287, 0.91438985,\n",
       "        0.91268898, 0.91347192, 0.9162527 , 0.91557775, 0.91576674,\n",
       "        0.91549676, 0.91530778, 0.91576674, 0.91449784, 0.91363391,\n",
       "        0.91433585, 0.91414687, 0.91409287, 0.91538877, 0.9149838 ,\n",
       "        0.91511879, 0.91487581, 0.91398488, 0.91528078, 0.91557775,\n",
       "        0.91441685, 0.91684665, 0.91741361, 0.91684665, 0.91792657,\n",
       "        0.91687365, 0.91703564, 0.91695464, 0.91525378, 0.91614471,\n",
       "        0.91609071, 0.91460583, 0.91595572, 0.91544276, 0.91528078,\n",
       "        0.91530778, 0.91646868, 0.91609071, 0.9149838 , 0.91511879,\n",
       "        0.91420086, 0.91544276, 0.9149838 , 0.91652268, 0.91479482,\n",
       "        0.91525378, 0.91665767, 0.91671166, 0.91590173, 0.9149568 ,\n",
       "        0.91671166, 0.91681965, 0.91816955, 0.91787257, 0.91735961,\n",
       "        0.91657667, 0.91590173, 0.91438985, 0.91425486, 0.91573974,\n",
       "        0.91633369, 0.91657667, 0.9174676 , 0.91833153, 0.91808855,\n",
       "        0.91725162, 0.91792657, 0.91703564, 0.91808855, 0.91887149,\n",
       "        0.9174406 , 0.91760259, 0.91857451, 0.91922246, 0.91868251,\n",
       "        0.91865551, 0.91957343, 0.91819654, 0.91922246, 0.91919546,\n",
       "        0.91954644, 0.92046436, 0.91957343, 0.91897948, 0.91957343,\n",
       "        0.91970842, 0.92043737, 0.92008639, 0.91949244, 0.91919546,\n",
       "        0.91927646, 0.91825054, 0.91976242, 0.91933045, 0.91970842,\n",
       "        0.91865551, 0.91733261, 0.91914147, 0.91868251, 0.91865551,\n",
       "        0.91906048, 0.91927646, 0.91962743, 0.92030238, 0.91968143,\n",
       "        0.92011339, 0.92057235, 0.92076134, 0.92065335, 0.92081533,\n",
       "        0.92011339, 0.91978942, 0.91954644, 0.91911447, 0.91846652,\n",
       "        0.91808855, 0.91673866, 0.91808855, 0.91830454, 0.91895248,\n",
       "        0.9187905 , 0.91862851, 0.91860151, 0.91846652, 0.91827754,\n",
       "        0.91854752, 0.91970842, 0.9212203 , 0.91976242, 0.91884449,\n",
       "        0.91965443, 0.91887149, 0.91941145, 0.91868251, 0.92019438,\n",
       "        0.91930346, 0.91962743, 0.91976242, 0.91965443, 0.92014039,\n",
       "        0.92078834, 0.92049136, 0.92051836, 0.92065335, 0.92068035,\n",
       "        0.92089633, 0.9211933 , 0.92086933, 0.91976242, 0.9200054 ,\n",
       "        0.92016739, 0.9200594 , 0.91916847, 0.91954644, 0.9212473 ,\n",
       "        0.92095032, 0.92065335, 0.91938445, 0.9200594 , 0.92032937,\n",
       "        0.9199784 , 0.91906048, 0.91984341, 0.91989741, 0.9212203 ,\n",
       "        0.91989741, 0.9200054 , 0.91987041, 0.91976242, 0.92113931,\n",
       "        0.92046436, 0.91895248, 0.91897948, 0.92049136, 0.92049136,\n",
       "        0.9213013 , 0.92097732, 0.92032937, 0.92111231, 0.92116631,\n",
       "        0.92073434, 0.9199784 , 0.91973542, 0.91933045, 0.92038337,\n",
       "        0.9199514 , 0.92070734, 0.92070734, 0.92068035, 0.92051836,\n",
       "        0.92165227, 0.92059935, 0.92149028, 0.91970842, 0.91987041,\n",
       "        0.92011339, 0.92068035, 0.92027538, 0.92059935, 0.92084233,\n",
       "        0.92038337, 0.91933045, 0.9200324 , 0.92011339, 0.9211933 ,\n",
       "        0.92165227, 0.92149028, 0.92189525, 0.92100432, 0.92138229,\n",
       "        0.9224892 , 0.92224622, 0.92232721, 0.92176026, 0.92111231,\n",
       "        0.92194924, 0.92235421, 0.92151728, 0.92197624, 0.92086933,\n",
       "        0.92162527, 0.92103132, 0.92059935, 0.91946544, 0.92024838,\n",
       "        0.92022138, 0.92043737, 0.92051836, 0.92103132, 0.92073434,\n",
       "        0.9213013 , 0.92135529, 0.92097732, 0.92103132, 0.92054536,\n",
       "        0.92105832, 0.9212473 , 0.92167927, 0.92111231, 0.92089633,\n",
       "        0.92184125, 0.92070734, 0.92046436, 0.92049136, 0.92070734,\n",
       "        0.92051836, 0.92165227, 0.92140929, 0.92089633, 0.92224622,\n",
       "        0.92197624, 0.92143629, 0.92181425, 0.92197624, 0.92157127,\n",
       "        0.92192225, 0.9213013 , 0.92081533, 0.92065335, 0.92051836,\n",
       "        0.92027538, 0.92140929, 0.92154428, 0.92135529, 0.9224892 ,\n",
       "        0.92308315, 0.92200324, 0.92275918, 0.92300216, 0.92216523,\n",
       "        0.92062635, 0.9211933 , 0.92189525, 0.92230022, 0.92219222,\n",
       "        0.92135529, 0.92173326, 0.92278618, 0.92289417, 0.92321814,\n",
       "        0.92240821, 0.92270518, 0.92243521, 0.92192225, 0.92262419,\n",
       "        0.92205724, 0.92235421, 0.92181425, 0.92176026, 0.92230022,\n",
       "        0.92162527, 0.92057235, 0.92105832, 0.92178726, 0.92300216,\n",
       "        0.92265119, 0.92308315, 0.92319114, 0.92308315, 0.9224892 ,\n",
       "        0.92181425, 0.92181425, 0.92181425, 0.92078834, 0.92095032,\n",
       "        0.92146328, 0.92208423, 0.92257019, 0.92213823, 0.92227322,\n",
       "        0.92170626, 0.92186825, 0.92230022, 0.92267819, 0.92197624,\n",
       "        0.92116631, 0.92200324, 0.92230022, 0.92165227, 0.92111231,\n",
       "        0.92189525, 0.92232721, 0.92162527, 0.92111231, 0.92211123,\n",
       "        0.92289417, 0.92273218, 0.92203024, 0.92232721, 0.92257019,\n",
       "        0.9225162 , 0.92281317, 0.92224622, 0.92178726, 0.92170626,\n",
       "        0.92165227, 0.92068035, 0.92073434, 0.92192225, 0.92257019,\n",
       "        0.92270518, 0.92275918, 0.92205724, 0.92186825, 0.92181425,\n",
       "        0.92243521, 0.92346112, 0.92232721, 0.92294816, 0.92205724,\n",
       "        0.92208423, 0.9225432 , 0.92367711, 0.9237041 , 0.92400108,\n",
       "        0.92338013, 0.9237581 , 0.92308315, 0.92275918, 0.92343413,\n",
       "        0.92292117, 0.92278618, 0.92281317, 0.92292117, 0.92284017,\n",
       "        0.92235421, 0.92286717, 0.92321814, 0.92200324, 0.92232721,\n",
       "        0.92346112, 0.92289417, 0.92305616, 0.92189525, 0.92205724,\n",
       "        0.92170626, 0.92159827, 0.9212203 , 0.92138229, 0.92165227,\n",
       "        0.92165227, 0.92157127, 0.92146328, 0.92273218, 0.92230022,\n",
       "        0.92273218, 0.92181425, 0.9224622 , 0.92265119, 0.92208423,\n",
       "        0.92270518, 0.92270518, 0.9225432 , 0.92186825, 0.92219222,\n",
       "        0.9224622 , 0.92216523, 0.92149028, 0.92302916, 0.92359611,\n",
       "        0.92313715, 0.92313715, 0.92257019, 0.92278618, 0.92275918,\n",
       "        0.92335313, 0.92335313, 0.92305616, 0.92292117, 0.92348812,\n",
       "        0.92197624, 0.92270518, 0.92359611, 0.92427106, 0.92394708,\n",
       "        0.92356911, 0.92343413, 0.92456803, 0.92397408, 0.92340713,\n",
       "        0.92278618, 0.92311015, 0.92227322, 0.92259719, 0.92157127,\n",
       "        0.92230022, 0.9224622 , 0.92286717, 0.92302916, 0.92281317,\n",
       "        0.92327214, 0.92235421, 0.92167927, 0.92297516, 0.92178726,\n",
       "        0.92305616, 0.92338013, 0.9237851 , 0.92270518, 0.92338013,\n",
       "        0.92362311, 0.92335313, 0.92354212, 0.92319114, 0.92278618,\n",
       "        0.92259719, 0.92221922, 0.92262419, 0.92392009, 0.92300216,\n",
       "        0.92365011, 0.92235421, 0.92203024, 0.92211123, 0.92270518,\n",
       "        0.92224622, 0.92181425, 0.92340713, 0.92324514, 0.92416307,\n",
       "        0.92343413, 0.92332613, 0.92332613, 0.92324514, 0.92365011,\n",
       "        0.9238121 , 0.92421706, 0.92392009, 0.9237311 , 0.92324514,\n",
       "        0.92305616, 0.9238121 , 0.92305616, 0.92338013, 0.92316415,\n",
       "        0.92240821, 0.92351512, 0.92421706, 0.92473002, 0.92392009,\n",
       "        0.9237041 , 0.92354212, 0.92292117, 0.92386609, 0.92389309,\n",
       "        0.92408207, 0.92354212, 0.92359611, 0.92308315, 0.92332613,\n",
       "        0.92294816, 0.92294816, 0.92362311, 0.92359611, 0.92405508,\n",
       "        0.92421706, 0.9237041 , 0.9224892 , 0.92338013, 0.92327214,\n",
       "        0.92343413, 0.92329914, 0.9237311 , 0.92359611, 0.92292117,\n",
       "        0.92302916, 0.92224622, 0.92294816, 0.92232721, 0.92362311,\n",
       "        0.92313715, 0.92302916, 0.92324514, 0.92316415, 0.92327214,\n",
       "        0.92284017, 0.92413607, 0.92394708, 0.92413607, 0.92294816,\n",
       "        0.92281317, 0.92348812, 0.92365011, 0.92302916, 0.92289417,\n",
       "        0.92227322, 0.92359611, 0.92262419, 0.92216523, 0.92243521,\n",
       "        0.92194924, 0.92338013, 0.92259719, 0.92270518, 0.9224892 ,\n",
       "        0.92300216, 0.92238121, 0.92300216, 0.92340713, 0.92327214,\n",
       "        0.9225162 , 0.9225432 , 0.92292117, 0.92243521, 0.92178726,\n",
       "        0.92200324, 0.92262419, 0.92289417, 0.92181425, 0.9211933 ,\n",
       "        0.92243521, 0.92365011, 0.92332613, 0.92362311, 0.92302916,\n",
       "        0.92346112, 0.92324514, 0.92270518, 0.92275918, 0.92313715,\n",
       "        0.92359611, 0.92346112, 0.92313715, 0.92351512, 0.92324514,\n",
       "        0.92437905, 0.92419006, 0.92302916, 0.92240821, 0.9224892 ,\n",
       "        0.92356911, 0.92221922, 0.92224622, 0.92232721, 0.92219222,\n",
       "        0.92292117, 0.92327214, 0.92289417, 0.9237311 , 0.92313715,\n",
       "        0.92243521, 0.92446004, 0.92389309, 0.92400108, 0.92419006,\n",
       "        0.92473002, 0.9237041 , 0.92367711, 0.92410907, 0.92348812,\n",
       "        0.92332613, 0.92362311, 0.92292117, 0.92400108, 0.92394708,\n",
       "        0.92354212, 0.92332613, 0.9238121 , 0.92329914, 0.92321814,\n",
       "        0.92394708, 0.92397408, 0.92305616, 0.92335313, 0.92273218,\n",
       "        0.92311015, 0.92289417, 0.9237581 , 0.92367711, 0.92408207,\n",
       "        0.92413607, 0.92413607, 0.92394708, 0.92362311, 0.92413607,\n",
       "        0.92429806, 0.92432505, 0.92402808, 0.92359611, 0.92362311,\n",
       "        0.92294816, 0.92270518, 0.92359611, 0.92338013, 0.92335313,\n",
       "        0.92316415, 0.92302916, 0.92408207, 0.92389309, 0.92383909,\n",
       "        0.92389309, 0.92451404, 0.92443305, 0.92405508, 0.92421706,\n",
       "        0.92459503, 0.92359611, 0.92416307, 0.92348812, 0.9237041 ,\n",
       "        0.92305616, 0.92292117, 0.92292117, 0.92335313, 0.92419006,\n",
       "        0.92408207, 0.92365011, 0.92473002, 0.925     , 0.924973  ,\n",
       "        0.92351512, 0.92324514, 0.9237851 , 0.92419006, 0.92324514,\n",
       "        0.92400108, 0.92402808, 0.92421706, 0.92456803, 0.92335313,\n",
       "        0.92329914, 0.92292117, 0.92348812, 0.92281317, 0.92327214,\n",
       "        0.92340713, 0.92289417, 0.92216523, 0.92294816, 0.92359611,\n",
       "        0.92413607, 0.92383909, 0.92392009, 0.92327214, 0.92219222,\n",
       "        0.9225432 , 0.92284017, 0.92300216, 0.9237311 , 0.92265119,\n",
       "        0.92227322, 0.92208423, 0.9224622 , 0.92316415, 0.92362311,\n",
       "        0.92413607, 0.92454104, 0.92475702, 0.92437905, 0.92354212,\n",
       "        0.92343413, 0.92383909, 0.92335313, 0.9237581 , 0.9237581 ,\n",
       "        0.92284017, 0.92302916, 0.92362311, 0.92362311, 0.92354212,\n",
       "        0.92362311, 0.9237311 , 0.92356911, 0.92340713, 0.92297516]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glob_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_training_data_value(num_users=100, loc_ep1=5, Numb_Glob_Iters=10, lamb=0, learning_rate=0,beta=0,algorithms=\"\", batch_size=0, dataset=\"\", k= 0 , personal_learning_rate =0 ,times = 5):\n",
    "    train_acc = np.zeros((times, Numb_Glob_Iters))\n",
    "    train_loss = np.zeros((times, Numb_Glob_Iters))\n",
    "    glob_acc = np.zeros((times, Numb_Glob_Iters))\n",
    "    algorithms_list  = [algorithms] * times\n",
    "    for i in range(times):\n",
    "        string_learning_rate = str(learning_rate)  \n",
    "        string_learning_rate = string_learning_rate + \"_\" +str(beta) + \"_\" +str(lamb)\n",
    "        if(algorithms == \"pFedMe\" or algorithms == \"pFedMe_p\"):\n",
    "            algorithms_list[i] = algorithms_list[i] + \"_\" + string_learning_rate + \"_\" + str(num_users) + \"u\" + \"_\" + str(batch_size) + \"b\" + \"_\" +str(loc_ep1) + \"_\"+ str(k)  + \"_\"+ str(personal_learning_rate) +  \"_\" +str(i) \n",
    "        else:\n",
    "            algorithms_list[i] = algorithms_list[i] + \"_\" + string_learning_rate + \"_\" + str(num_users) + \"u\" + \"_\" + str(batch_size) + \"b\"  \"_\" +str(loc_ep1) +  \"_\" +str(i)\n",
    "\n",
    "        train_acc[i, :], train_loss[i, :], glob_acc[i, :] = np.array(\n",
    "            simple_read_data(dataset +\"_\"+ algorithms_list[i]))[:, :Numb_Glob_Iters]\n",
    "    return glob_acc, train_acc, train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithms = \"FedAvg\"\n",
    "dataset = \"Mnist\"\n",
    "learning_rates = 0.02\n",
    "betas = 1.0\n",
    "lambdas = 15\n",
    "batch_sizes = 20\n",
    "num_users = 5\n",
    "local_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mnist_FedAvg_0.02_1.0_15_5u_20b_20_0\n",
      "Mnist_FedAvg_0.02_1.0_15_5u_20b_20_1\n",
      "Mnist_FedAvg_0.02_1.0_15_5u_20b_20_2\n",
      "Mnist_FedAvg_0.02_1.0_15_5u_20b_20_3\n",
      "Mnist_FedAvg_0.02_1.0_15_5u_20b_20_4\n",
      "Mnist_FedAvg_0.02_1.0_15_5u_20b_20_5\n",
      "Mnist_FedAvg_0.02_1.0_15_5u_20b_20_6\n",
      "Mnist_FedAvg_0.02_1.0_15_5u_20b_20_7\n",
      "Mnist_FedAvg_0.02_1.0_15_5u_20b_20_8\n",
      "Mnist_FedAvg_0.02_1.0_15_5u_20b_20_9\n"
     ]
    }
   ],
   "source": [
    "glob_acc_all, train_acc_all, train_loss_all = get_all_training_data_value(num_users=num_users,\n",
    "                            loc_ep1=local_epochs,\n",
    "                            Numb_Glob_Iters=800,\n",
    "                            lamb=lambdas,\n",
    "                            learning_rate=learning_rates,\n",
    "                            beta=betas,\n",
    "                            algorithms=algorithms,\n",
    "                            batch_size=batch_sizes,\n",
    "                            dataset=dataset,\n",
    "                            times=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.08288191, 0.40068617, 0.47950524, ..., 0.96478873, 0.96117732,\n",
       "        0.96451788],\n",
       "       [0.08288191, 0.30371975, 0.30516432, ..., 0.96325388, 0.96262189,\n",
       "        0.96126761],\n",
       "       [0.08288191, 0.281329  , 0.34344529, ..., 0.96424702, 0.9625316 ,\n",
       "        0.96162875],\n",
       "       ...,\n",
       "       [0.08288191, 0.51155652, 0.41251354, ..., 0.96271217, 0.96325388,\n",
       "        0.96325388],\n",
       "       [0.08288191, 0.27103648, 0.57611051, ..., 0.96171903, 0.96153846,\n",
       "        0.96217046],\n",
       "       [0.08288191, 0.39942217, 0.51399422, ..., 0.96171903, 0.9618996 ,\n",
       "        0.96072589]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_acc_all"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
